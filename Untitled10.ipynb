{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#                         Assignment-\n",
        "#Supervised Learning: Regression Models and Performance Metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "PD6gula1BcNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1-  What is Simple Linear Regression (SLR)? Explain its purpose\n",
        " >   Simple Linear Regression (SLR) is a statistical technique used to study the relationship between two variables — one independent variable (X) and one dependent variable (Y). It fits a straight line through the data points to predict the value of Y based on X. The main goal of SLR is to understand how changes in the independent variable affect the dependent variable. It helps in forecasting, trend analysis, and decision-making. For example, predicting a person’s weight based on their height. The fitted line is usually expressed as\n",
        "𝑌\n",
        "=\n",
        "𝑎\n",
        "+\n",
        "𝑏\n",
        "𝑋\n",
        "+\n",
        "𝜀\n",
        "Y=a+bX+ε, where\n",
        "𝑎\n",
        "a is the intercept,\n",
        "𝑏\n",
        "b is the slope, and\n",
        "𝜀\n",
        "ε is the error term."
      ],
      "metadata": {
        "id": "_5GI-Fox3ff1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 - What are the key assumptions of simple linear Regression?\n",
        "   - Here are the key assumptions of simple linear regression:\n",
        "\n",
        "1.  **Linearity:** The relationship between the independent variable (X) and the dependent variable (Y) must be linear. This means the data points should roughly form a straight line when plotted.\n",
        "\n",
        "2.  **Independence of Errors:** The errors (residuals) should be independent of each other. This means that the error for one data point should not be related to the error for any other data point.\n",
        "\n",
        "3.  **Homoscedasticity (Constant Variance of Errors):** The variance of the errors should be constant across all levels of the independent variable. This means the spread of the residuals should be roughly the same throughout the range of X values.\n",
        "\n",
        "4.  **Normality of Errors:** The errors (residuals) should be approximately normally distributed. This assumption is more important for smaller sample sizes and for making inferences about the population parameters (like confidence intervals and p-values).\n",
        "\n",
        "5.  **No Multicollinearity:** While simple linear regression only involves one independent variable, this assumption is crucial in multiple linear regression and states that the independent variables should not be highly correlated with each other."
      ],
      "metadata": {
        "id": "h9VcmyMUAV9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Q3 – Write the mathematical equation for a simple linear regression model and explain each term\n",
        "  - The mathematical equation for Simple Linear Regression is:\n",
        "\n",
        "      𝑌=𝑎+𝑏𝑋+\n",
        "        Y=a+bX+ε\n",
        "\n",
        "     Where:\n",
        "\n",
        "     Y = Dependent variable (the outcome or value we want to predict)\n",
        "\n",
        "     X = Independent variable (the input or predictor)\n",
        "\n",
        "     a = Intercept (the value of Y when X = 0)\n",
        "\n",
        "     b = Slope (the rate of change in Y for a one-unit increase in X)\n",
        "\n",
        "     ε (epsilon) = Error term (the difference between actual and predicted values)\n",
        "\n",
        "     This equation defines a straight line that best fits the relationship between X and Y, minimizing the error between observed and predicted values."
      ],
      "metadata": {
        "id": "5XngP6UW6gPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 – Provide a real-world example where simple linear regression can be applied\n",
        "   -  A common real-world example of Simple Linear Regression is predicting a person’s weight based on their height. As height increases, weight generally tends to increase in a roughly linear pattern. By collecting data from several individuals, we can build a regression model to estimate weight (dependent variable) from height (independent variable). Similarly, it can be used to predict house prices based on size, sales revenue based on advertising spend, or temperature effects on electricity consumption. This helps in forecasting and understanding relationships between two measurable variables."
      ],
      "metadata": {
        "id": "RtiyVkgr7lts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5 – What is the method of least squares in linear regression?\n",
        "  -  The method of least squares is a mathematical approach used to find the best-fitting line in a regression model. It works by minimizing the sum of the squares of the differences between the actual data points and the values predicted by the line. These differences are called residuals or errors. By squaring them, both positive and negative errors are treated equally. The line with the smallest total squared error is considered the “best fit.” This method ensures the most accurate representation of the relationship between the independent and dependent variables."
      ],
      "metadata": {
        "id": "qxcXZ6dq7vdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6 – What is Logistic Regression? How does it differ from Linear Regression?\n",
        "  -  Logistic Regression is a supervised learning algorithm used for classification problems, where the output variable is categorical (e.g., Yes/No, 0/1, True/False). It predicts the probability that an observation belongs to a particular class using the sigmoid (logistic) function, which outputs values between 0 and 1.\n",
        "\n",
        "  The main difference from Linear Regression is that linear regression predicts continuous values, while logistic regression predicts probabilities or binary outcomes. Linear regression uses a straight-line relationship, whereas logistic regression uses an S-shaped curve to model the data."
      ],
      "metadata": {
        "id": "MOYr3QGN756T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7 – Name and briefly describe three common evaluation metrics for regression models\n",
        "   - Ans\n",
        "     1-Mean Absolute Error (MAE): It measures the average absolute difference between predicted and actual values. Lower MAE means better accuracy.\n",
        "\n",
        "     2- Mean Squared Error (MSE): It calculates the average of the squared differences between predicted and actual values. It penalizes larger errors more strongly.\n",
        "\n",
        "     3- R-squared (Coefficient of Determination): It indicates how well the independent variable explains the variation in the dependent variable. An R² value closer to 1 means a better model fit."
      ],
      "metadata": {
        "id": "f4sWYyC38Ja5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8 – What is the purpose of the R-squared metric in regression analysis?\n",
        "  -  The R-squared (R²) metric measures how well the independent variable(s) explain the variation in the dependent variable. It represents the proportion of variance in the dependent variable that can be predicted from the independent variable. R² values range from 0 to 1, where 0 means the model explains none of the variation, and 1 means it explains all the variation. A higher R² indicates a better fit of the model to the data. However, it doesn’t indicate whether the model is biased or if it fits new data well."
      ],
      "metadata": {
        "id": "_YCH6zdj8-P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9 – Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept)"
      ],
      "metadata": {
        "id": "OwTencTs9cza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)   # Independent variable\n",
        "y = np.array([2, 4, 5, 4, 5])                  # Dependent variable\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print slope and intercept\n",
        "print(\"Slope (Coefficient):\", model.coef_[0])\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhdA6f89jh4",
        "outputId": "0463d083-aee0-4cf4-bd13-110c021ddd81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (Coefficient): 0.6\n",
            "Intercept: 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10 - How do you interpret the coefficients in a simple linear regression model?\n",
        "  -   In a simple linear regression model, the coefficients represent the following:\n",
        "\n",
        "      Intercept (a): This is the predicted value of the dependent variable (Y) when the independent variable (X) is zero. It's the point where the regression line crosses the Y-axis. In some cases, it might not have a meaningful interpretation if X cannot be zero in the real world.\n",
        "\n",
        "      Slope (b): This represents the change in the dependent variable (Y) for a one-unit increase in the independent variable (X). It indicates the direction and strength of the linear relationship between the two variables. A positive slope means that as X increases, Y tends to increase. A negative slope means that as X increases, Y tends to decrease. The magnitude of the slope indicates how much Y changes for each unit change in X.\n",
        "\n",
        "      For example, in the code provided, the slope is 0.6 and the intercept is 2.2. This means that for every one-unit increase in X, Y is predicted to increase by 0.6 units. When X is 0, Y is predicted to be 2.2.\n",
        "\n"
      ],
      "metadata": {
        "id": "IcDGmy9p-Fn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jKchBcxn_uPd"
      }
    }
  ]
}